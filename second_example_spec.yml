# ----------------------------------------------------------
# EXHIBIT SPECIFICATION
# =====================
#
# This specification describes the dataset in great detail.
# In order to vary the degree to which it is anonymised,
# please review each section and make necessary adjustments.
# ----------------------------------------------------------
metadata:
  number_of_rows: 100
  uuid_columns:
  - id
  - chi
  categorical_columns:
  - gender
  - consent
  - year
  - school_year
  numerical_columns:
  date_columns:
  - date_created
  - date_updated
  geospatial_columns:
  inline_limit: 15
  random_seed: 0
  id: demo_two
# ----------------------------------------------------------
# COLUMN DETAILS
# ==============
#
# Dataset columns are categorised into one of the three types: 
# Categorical | Continuous | Date
#
# Column type determines what parameters are included in the
# specification. When making changes to the values, please
# note their format. Values starting with a number must be
# enclosed in quotes as per YAML rules.
# 
# CATEGORICAL COLUMNS
# -----------
# The default anonymising method for categorical columns is
# "random", meaning original values are drawn at random,
# (respecting probabilities, if supplied) but you can add
# your own custom sets, including linked, by creating a
# suitable table in the exhibit database (local or external).
#
# The tool comes with a number of sample anonymising sets
# (see documentation). To use just one column from a set,
# add a dot separator like so mountains.range
# 
# You can also use a subset of regular expression patterns 
# to generate aliasing values in non-linked columns. For example,
# if your confidential data had Consultant GMC numbers, you can
# anonymise them using a regex pattern GMC[0-9]{5}[XY] which
# will generate values like GMC00000X.
#
# Depending on the number of unique values in a column,
# its original values can either be listed in the spec,
# put into the exhibit database or, if the values follow a
# one to one relationship with another column, be listed
# as part of that column's section.
#
# UUID COLUMNS
# -----------
# UUID columns are a special case for categorical column where
# each value is unique, but can appear multiple times depending
# on the frequency distribution set by the user. You can add UUID
# columns manually or infer them from the source data. If adding a
# UUID column manually, don't forget to add it in the metadata section.
#
# The format of a UUID column in the specification is as follows. Note
# that there is an optional "uuid_seed" parameter for cases when you want
# to have differently seeded uuids in the same spec.
# 
# record_chi:
#  type: uuid
#  frequency_distribution:
#  - frequency | probability_vector
#  - 1         | 0.5
#  - 2         | 0.3
#  - 3         | 0.2
#  miss_probability: 0.0
#  anonymising_set: uuid
#
# You can choose between uuid, range and pseudo_chi anonymising sets.
# ----------------------------------------------------------
columns:
  id:
    type: uuid
    uuid_seed: 0
    frequency_distribution:
    - frequency | probability_vector
    - 1         | 1.0
    miss_probability: 0.0
    anonymising_set: uuid
  chi:
    type: uuid
    uuid_seed: 0
    frequency_distribution:
    - frequency | probability_vector
    - 1         | 1.0
    miss_probability: 0.0
    anonymising_set: pseudo_chi
  gender:
    type: categorical
    name: consent
    original_values:
    - gender             | probability_vector |
    - F                  | 0.9                |
    - M                  | 0.1                |
    - Missing data       | 0.000              |
    paired_columns:
    uniques: 2
    cross_join_all_unique_values: false
    miss_probability: 0.0
    anonymising_set: random
    dispersion: 0
  consent:
    type: categorical
    name: consent
    original_values:
    - consent            | probability_vector |
    - Y                  | 0.9                |
    - N                  | 0.1                |
    - Missing data       | 0.000              |
    paired_columns:
    uniques: 2
    cross_join_all_unique_values: false
    miss_probability: 0.0
    anonymising_set: random
    dispersion: 0
  year:
    type: categorical
    name: year
    original_values:
    - year  | probability_vector |
    - P1          | 0.16               |
    - P2          | 0.07               |
    - P3          | 0.07               |
    - P4          | 0.07               |
    - P5          | 0.07               |
    - P6          | 0.07               |
    - P7          | 0.07               |
    - S1          | 0.07               |
    - S2          | 0.07               |
    - S3          | 0.07               |
    - S4          | 0.07               |
    - S5          | 0.07               |
    - S6          | 0.07               |
    - Missing data| 0.00               |
    paired_columns:
    uniques: 13
    cross_join_all_unique_values: false
    miss_probability: 0.0
    anonymising_set: random
    dispersion: 0
  school_year:
    type: categorical
    name: school_year
    original_values:
    - school_year  | probability_vector |
    - 2023/2024    | 1.000              |
    - Missing data | 0.000              |
    paired_columns:
    uniques: 1
    cross_join_all_unique_values: false
    miss_probability: 0.0
    anonymising_set: random
    dispersion: 0
# ----------------------------------------------------------
# DATE COLUMNS
# ----------
# Exhibit will try to determine date columns automatically, but
# you can also add them manually, providing the following parameters:
#   type: date
#   cross_join_all_unique_values: true | false
#   miss_probability: between 0 and 1
#   from: 'yyyy-mm-dd'
#   uniques: 4
#   frequency: QS
# 
# Frequency is based on the frequency strings of DateOffsets.
# See Pandas documentation for more details. Times are not supported.
# ----------------------------------------------------------
# GEOSPATIAL COLUMNS
# ----------
#
# Geospatial columns are special in that they are not inferred
# from source data and can only be added manually. The format 
# of each column is as follows:
#
# clinic_coords:
#    type: geospatial
#    h3_table: geo_scotland_dz_h3_8
#    distribution: uniform
#    miss_probability: 0
#
# h3_table refers to H3 hexagon IDs stored in the exhibit DB. exhibit
# doesn't come with any geospatial lookups by default, but there
# is a recipe explaining how to create one. The format of the table
# is that it has to have a column named "h3" with h3 ids. 
# 
# For distribution, you can either pick uniform to sample points
# from all hexagons at random or use column weights in the h3_table,
# like population counts.
# ----------------------------------------------------------
  date_created:
    type: date
    from: '2024-01-01'
    to: '2024-01-31'
    uniques: 20
    frequency: null
    cross_join_all_unique_values: false
    miss_probability: 0.0
  date_updated:
    type: date
    from: '2024-01-01'
    to: '2024-01-31'
    uniques: 20
    frequency: null
    cross_join_all_unique_values: false
    miss_probability: 0.0
# ----------------------------------------------------------
# CONSTRAINTS
# ===========
#
# There are two types of constraints you can impose of the data:
# - basic
# - custom
# 
# Basic constraints take the form of a simple statement of the
# form [dependent_column] [operator] [expression / independent_column].
# The tool will try to guess these relationships when creating a
# spec. You can also force a column to be always smaller / larger
# than a scalar value. Note that adding a basic constraint between
# two columns will affect the distribution of weights and also the
# target sum as these are designed to work with a single column.
#
# Custom constraints are more flexible and can target specific
# subsets of values with different actions. Currently the following
# actions are supported:
#
# - "make_null"
# - "make_not_null"
# - "make_outlier"
# - "sort_ascending"
# - "sort_descending"
# - "make_distinct"
# - "make_same"
# - "make_almost_same"
# - "generate_as_sequence"
# - "generate_as_repeating_sequence"
# - "geo_make_regions"
# - "sort_and_skew_left"
# - "sort_and_skew_right"
# - "sort_and_make_peak"
# - "sort_and_make_valley"
# - "shift_distribution_right"
# - "shift_distribution_left"
# - "assign_value"
#
# assign_value can take an argument separated by $ (assign_value$hello_world)
# which will override the synthesised value in the filter selection.                   
#
# Adding or banning nulls is useful when a value in one column, 
# like Readmissions Within 28 days, necessitates a valid value in
# another, like Readmissions Within 7 days.
# 
# The format for custom constraints is as follows:
#
# demo_constraint_name:
#   filter: (los > 2)
#   partition: age, sex
#   targets:
#     taget_column: target_action
#
# Custom constraints can target multiple columns, but each column can
# appear only once in each custom constraint.
# 
# Expressions used in the filter must be understood by Pandas eval().
# Additionally, you can use these custom filters:
#
# - "COLUMN_NAME with_high_frequency"
# - "COLUMN_NAME with_low_frequency"
#
# If a column name has spaces, make sure to surround it with
# the tilde character ~. This rule only applies when columns are used
# in basic constraints or filters. Targets for custom constraints must 
# use column names as they are. When comparing a date column against a
# fixed date, make sure it's in an ISO format and is enclosed in
# single quotation marks like so '2018-12-01'.
# ----------------------------------------------------------
constraints:
  allow_duplicates: false
  basic_constraints:
    - date_updated == date_created
  custom_constraints:
# ----------------------------------------------------------
# LINKED COLUMNS
# ===============
#
# There are two types of linked groups - those manually defined using
# the --linked_columns (or -lc) command line parameter and automatically
# detected groups where columns follow a hierarchical (one to many)
# relationship. User defined linked columns are always put under the
# zero indexed group.
#
# Groups of hierarchically linked columns are listed together under the 
# index starting from 1. Their details are saved in the exhibit database.
#
# The specification format is as follows:
# - - 1
#   - - Parent column
#   - - Child column
# - - 2
#   - - ...etc.
# It's possible to add a linked columns group manually by adding 
# a table to the exhibit databse with the hierarchical relationships.
# The name of this table must follow the format: id_N  where id is 
# taken from the metadata section and N is the group number.
# ----------------------------------------------------------
linked_columns:
# ----------------------------------------------------------
# DERIVED COLUMNS
# ===============
#
# You can add columns that will be calculated after the rest
# of the dataset has been generated.
#
# The calculation should be in a format that Pandas' eval()
# method can parse and understand. 
#
# For example, assuming you have Numerator column A and
# Denominator column B, you would write Rate: (A / B)
#
# You can also add a derived column with the current date and
# time by using a special variable '@sysdate'
# ----------------------------------------------------------
derived_columns:
# ----------------------------------------------------------
# MODELS
# ===============
# 
# You can generate columns in your dataset using custom machine
# learning models, using the synthetic dataset generated so far
# as input. Follow the tutorial in the recipes folder to create
# your model and save it in the models folder. The specification
# format is as follows:
# 
# models:
#   model_name (without .pickle extension):
#     hyperparameter_name: hyperparameter_value
# 
# You can chain models one after another - they are called in the
# same order as they appear in the specification. Make sure that
# the same libraries used in creating the model are available in
# the environment where Exhibit is installed.
# ----------------------------------------------------------
models:
